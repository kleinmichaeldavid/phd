{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Hashtag and Tweet Searcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import re\n",
    "import sqlite3\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import threading\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import compress\n",
    "from time import time, sleep\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.units import inch, cm\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.pdfbase.pdfmetrics import stringWidth\n",
    "from reportlab.platypus import Image\n",
    "from math import inf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables that contains the user credentials to access Twitter API \n",
    "access_token = 'xxx'\n",
    "access_token_secret = 'xxx'\n",
    "consumer_key = 'xxx'\n",
    "consumer_secret = 'xxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Finding Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_woeids():\n",
    "    \n",
    "    ##\n",
    "    ## Returns two lists: the names and woeids for the \n",
    "    ## US cities available through the trends api\n",
    "    ##\n",
    "    \n",
    "    avails = api.trends_available()\n",
    "    is_us = [a['country'] == 'United States' and a['name'] != 'United States' for a in avails]\n",
    "    us_avails = list(compress(avails, is_us))\n",
    "    places = [a['name'] for a in us_avails]\n",
    "    woeids = [a['woeid'] for a in us_avails]\n",
    "    return(places, woeids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hashtags(places, woeids, ht_filename, i, cycle):\n",
    "    \n",
    "    ##\n",
    "    ## Gets trending hashtags for each of the places in\n",
    "    ## 'woeid'. Saves them in a csv.\n",
    "    ##\n",
    "    \n",
    "    search = []\n",
    "    for woeid in woeids:\n",
    "        city_trends = api.trends_place(woeid)\n",
    "        queries = [trend['query'] for trend in city_trends[0]['trends']]\n",
    "        search.append(queries)\n",
    "\n",
    "    # write results\n",
    "    search_df = pd.DataFrame(search).T\n",
    "    search_df.columns = places\n",
    "    search_df.to_csv(ht_filename + str(cycle) + '_' + str(i) + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeated_finder(places, woeids, ht_filename, reps, interval, cycle):\n",
    "    \n",
    "    ##\n",
    "    ## Runs 'find_hashtags' 'reps' times\n",
    "    ## with 'interval' between each run.\n",
    "    ##\n",
    "    \n",
    "    for i in range(reps):\n",
    "        start_time = time()\n",
    "        find_hashtags(places, woeids, ht_filename, i, cycle)\n",
    "        if i < (reps - 1):\n",
    "            remaining = time() - start_time\n",
    "            print('sleeping')\n",
    "            sleep(interval - remaining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Scoring / Filtering Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer(row):\n",
    "    return(2 ** (-row/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_hashtags(repeats, ht_filename, hash_samples, cycle):\n",
    "    \n",
    "    ##\n",
    "    ## Reads files from 'repeated_finder', scores the\n",
    "    ## hashtags, saves the scores as a csv and\n",
    "    ## outputs a list of the top 'hash_samples' hashtags.\n",
    "    ##\n",
    "    ## Note: if cycle > 0, checks whether any of the hashtags\n",
    "    ## have already appeared in past lists and filters those.\n",
    "    ##\n",
    "    \n",
    "    ## iterate through each city and repetition\n",
    "    tagdict = {}\n",
    "    for i in range(reps):\n",
    "        data = pd.read_csv(ht_filename + str(cycle) + '_' + str(i) + '.csv')\n",
    "\n",
    "        for city in data:\n",
    "            for row,tag in enumerate(data[city]):\n",
    "                lowered = str(tag).lower()\n",
    "                if lowered not in tagdict:\n",
    "                    tagdict[lowered] = scorer(row)\n",
    "                else:\n",
    "                    tagdict[lowered] += scorer(row)\n",
    "                    \n",
    "    keys = []\n",
    "    values = []\n",
    "    for key,value in tagdict.items():\n",
    "        keys.append(key)\n",
    "        values.append(value)\n",
    "    \n",
    "    # create df with potential hashtags and their scores\n",
    "    tag_df = pd.DataFrame({'hashtag':keys,'score':values})\n",
    "    tag_df = tag_df[tag_df.hashtag != 'nan']\n",
    "    tag_df = tag_df.sort_values(by = 'score', ascending = False)\n",
    "    tag_filter = [True if re.search('%23',q) else False for q in tag_df['hashtag']]\n",
    "    tag_df = tag_df[tag_filter].reset_index(drop = True)\n",
    "    \n",
    "    \n",
    "    if cycle == 0:\n",
    "        tag_df[:hash_samples].to_csv('previous_' + ht_filename + '.csv', index = False)\n",
    "\n",
    "    if cycle > 0:\n",
    "        prev_tags = pd.read_csv('previous_' + ht_filename + '.csv') \n",
    "        tag_df = tag_df[-tag_df['hashtag'].isin(prev_tags['hashtag'])].reset_index(drop = True)\n",
    "        tag_df[:hash_samples].to_csv('previous_' + ht_filename + '.csv', mode='a', header=False, index = False)\n",
    "    \n",
    "    \n",
    "    return(list(tag_df['hashtag'][:hash_samples]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Searching Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(db_name, query):\n",
    "    \n",
    "    ##\n",
    "    ## Connects to sqlite3 database (creates one if neccessary)\n",
    "    ## and creates a new table for the given hashtag.\n",
    "    ##\n",
    "    \n",
    "    table_name = 'hashtag_' + re.sub('[^a-zA-Z0-9]','',query)\n",
    "    \n",
    "    # create and connect to database\n",
    "    sqlite_file = db_name + '.sqlite'\n",
    "    conn = sqlite3.connect(sqlite_file)\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # create table\n",
    "    c.execute('CREATE TABLE {qry}(\\\n",
    "    tweet_id INTEGER PRIMARY KEY,\\\n",
    "    datetime TEXT,\\\n",
    "    source TEXT,\\\n",
    "    user_id TEXT,\\\n",
    "    user_name TEXT,\\\n",
    "    user_location TEXT,\\\n",
    "    user_description TEXT,\\\n",
    "    user_followers TEXT,\\\n",
    "    user_friends TEXT,\\\n",
    "    user_acc_age TEXT,\\\n",
    "    user_verified TEXT,\\\n",
    "    user_statuses TEXT,\\\n",
    "    user_offset TEXT,\\\n",
    "    user_lang TEXT,\\\n",
    "    is_quote_status TEXT,\\\n",
    "    favorited TEXT,\\\n",
    "    retweeted TEXT,\\\n",
    "    language TEXT,\\\n",
    "    mentions TEXT,\\\n",
    "    reply_to TEXT,\\\n",
    "    is_retweet TEXT,\\\n",
    "    retweet_text TEXT,\\\n",
    "    retweet_user TEXT,\\\n",
    "    text TEXT)'\\\n",
    "        .format(qry=table_name))\n",
    "    \n",
    "    conn.commit()\n",
    "    \n",
    "    return(c,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cursor(queries,max_tweets):\n",
    "    \n",
    "    ##\n",
    "    ## Creates tweepy cursor to search tweets.\n",
    "    ##\n",
    "    \n",
    "    cursor = tweepy.Cursor(api.search,\n",
    "                           count=100,\n",
    "                           q=queries,\n",
    "                           tweet_mode='extended').items(max_tweets)\n",
    "    return(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_tweets(query,cursor,c,conn,api,fullTime):\n",
    "    \n",
    "    ##\n",
    "    ## Iterates through the cursor from 'create_cursor'\n",
    "    ## and adds tweets to the database.\n",
    "    ##\n",
    "    \n",
    "    \n",
    "    table_name = 'hashtag_' + re.sub('[^a-zA-Z0-9]','',query)\n",
    "    \n",
    "    tweetCount = 0\n",
    "    tweet_set = []\n",
    "\n",
    "    iterobject = iter(cursor)\n",
    "    \n",
    "    while iterobject:\n",
    "        try:\n",
    "            tweet = next(iterobject)._json\n",
    "            tweetCount += 1\n",
    "        \n",
    "            tweet_id = tweet['id_str']\n",
    "            datetime = tweet['created_at']\n",
    "            source = tweet['source']\n",
    "            user_id = tweet['user']['id_str']\n",
    "            user_name = tweet['user']['screen_name']\n",
    "            user_location = tweet['user']['location']\n",
    "            user_description = tweet['user']['description']\n",
    "            user_followers = tweet['user']['followers_count']\n",
    "            user_friends = tweet['user']['friends_count']\n",
    "            user_acc_age = tweet['user']['created_at']\n",
    "            user_verified = tweet['user']['verified']\n",
    "            user_statuses = tweet['user']['statuses_count']\n",
    "            user_offset = tweet['user']['utc_offset']\n",
    "            user_lang = tweet['user']['lang']\n",
    "            is_quote_status = tweet['is_quote_status']\n",
    "            favorited = tweet['favorited']\n",
    "            retweeted = tweet['retweeted']\n",
    "            language = tweet['lang']\n",
    "            if 'user_mentions' in tweet['entities']['user_mentions']:\n",
    "                mentions = tweet['entities']['user_mentions']['id_str']\n",
    "            else:\n",
    "                mentions = None\n",
    "    \n",
    "            if 'in_reply_to_user_id_str' in tweet:\n",
    "                reply_to = tweet['in_reply_to_user_id_str']\n",
    "            else:\n",
    "                reply_to = None\n",
    "    \n",
    "            if 'retweeted_status' in tweet:\n",
    "                is_retweet = 'yes'\n",
    "                retweet_text = tweet['retweeted_status']['full_text']\n",
    "                retweet_user = tweet['retweeted_status']['user']['id_str']\n",
    "            else:\n",
    "                is_retweet = 'no'\n",
    "                retweet_text = None\n",
    "                retweet_user = None\n",
    "\n",
    "            if 'text' in tweet:\n",
    "                text = tweet['text']\n",
    "       \n",
    "            if 'full_text' in tweet:\n",
    "                text = tweet['full_text']\n",
    "    \n",
    "            # put it all together\n",
    "            tweet_set.append((tweet_id, datetime, source, user_id, user_name,\n",
    "                    user_location,user_location,user_description,\n",
    "                    user_followers,user_friends,user_acc_age,\n",
    "                    user_verified,user_statuses,user_offset,\n",
    "                    user_lang,is_quote_status,favorited,\n",
    "                    retweeted,language,is_retweet,\n",
    "                    mentions,reply_to,retweet_text,\n",
    "                    retweet_user,text))\n",
    "\n",
    "            if tweetCount % 100 == 0:\n",
    "                rls = api.rate_limit_status()\n",
    "                limits_left = rls['resources']['application']['/application/rate_limit_status']['remaining']\n",
    "                searches_left = rls['resources']['search']['/search/tweets']['remaining']\n",
    "    \n",
    "                if limits_left < 3 or searches_left < 3:\n",
    "           \n",
    "                    c.executemany('INSERT OR IGNORE INTO {qry}\\\n",
    "                    (tweet_id, datetime, source, user_id, user_name,\\\n",
    "                    user_location,user_location,user_description,\\\n",
    "                    user_followers,user_friends,user_acc_age,\\\n",
    "                    user_verified,user_statuses,user_offset,\\\n",
    "                    user_lang,is_quote_status,favorited,\\\n",
    "                    retweeted,language,is_retweet,\\\n",
    "                    mentions,reply_to,retweet_text,\\\n",
    "                    retweet_user,text)\\\n",
    "                    VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,\\\n",
    "                    ?,?,?,?,?,?,?,?,?,?,?)'.format(qry=table_name),\n",
    "                                  tweet_set)\n",
    "                    conn.commit()\n",
    "            \n",
    "                    tweet_set = []\n",
    "            \n",
    "                    snooze = max([30 + 15 * 60 - (time() - fullTime),0]) ### want to inherite time from last search\n",
    "                    print('Snoozing for ',snooze,', with ',tweetCount,' completed so far.')\n",
    "                    sleep(snooze)\n",
    "                    fullTime = time()\n",
    "                    print('back to searching')\n",
    "    \n",
    "        except StopIteration:\n",
    "            print('end of hash')\n",
    "            break\n",
    "        except:\n",
    "            print('oops')\n",
    "            continue\n",
    "\n",
    "        \n",
    "\n",
    "    # at the end of the cursor\n",
    "    if len(tweet_set) > 0:\n",
    "        c.executemany('INSERT OR IGNORE INTO {qry}\\\n",
    "        (tweet_id, datetime, source, user_id, user_name,\\\n",
    "        user_location,user_location,user_description,\\\n",
    "        user_followers,user_friends,user_acc_age,\\\n",
    "        user_verified,user_statuses,user_offset,\\\n",
    "        user_lang,is_quote_status,favorited,\\\n",
    "        retweeted,language,is_retweet,\\\n",
    "        mentions,reply_to,retweet_text,\\\n",
    "        retweet_user,text)\\\n",
    "        VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,\\\n",
    "        ?,?,?,?,?,?,?,?,?,?,?)'.format(qry=table_name),\n",
    "                      tweet_set)\n",
    "        conn.commit()\n",
    "    \n",
    "    conn.close()\n",
    "    print('done!')\n",
    "    return(time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(queries,db_name,tweet_samples):\n",
    "    \n",
    "    ##\n",
    "    ## Loops through the search functions for all hashtags\n",
    "    ##\n",
    "    \n",
    "    current_time = time()\n",
    "    for i,q in enumerate(queries):\n",
    "        print('searching for...' + q)\n",
    "        c,conn = create_db(db_name, q[3:])\n",
    "        cursor = create_cursor(q,tweet_samples)\n",
    "        current_time = pull_tweets(q[3:],cursor,c,conn,api,current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_table(c,query):\n",
    "    c.execute('SELECT * FROM {tn} WHERE {cn}=\"no\"'.\\\n",
    "        format(coi='id', tn=query, cn='is_retweet'))\n",
    "\n",
    "    check = c.fetchall()\n",
    "    df = pd.DataFrame(check)\n",
    "    df.columns = ['tweet_id', 'datetime', 'source', 'user_id',\n",
    "                  'user_name','user_location',\n",
    "                  'user_description','user_followers',\n",
    "                  'user_friends','user_acc_age',\n",
    "                  'user_verified','user_statuses','user_offset',\n",
    "                  'user_lang','is_quote_status','favorited',\n",
    "                  'retweeted','language','is_retweet',\n",
    "                  'mentions','reply_to','retweet_text',\n",
    "                  'retweet_user','text']\n",
    "\n",
    "    return(df)\n",
    "\n",
    "def trim_data(df,items):\n",
    "    df = df[df['language'] == 'en']\n",
    "    df = df[df['user_followers'] != '0']\n",
    "    df = df.filter(items=items)\n",
    "    return(df)\n",
    "\n",
    "def clean_text(text):\n",
    "    clean_tweets = []\n",
    "    for tweet in text:\n",
    "        clean = tweet.lower() # set to lowercase\n",
    "        clean = re.sub('\\n',' ',clean) # change newline to space\n",
    "        clean = re.sub('&amp','&',clean) # fix ampersands\n",
    "        clean = re.sub('#[^ ]*','',clean) # remove hashtags\n",
    "        clean = re.sub('@[^ ]*','',clean) # remove mentions\n",
    "        clean = re.sub('http[^ ]*','',clean) # remove links\n",
    "        clean = re.sub('[^a-z0-9 ]','',clean) # remove non-alphanumerics\n",
    "        clean = re.sub(' + ',' ',clean) # multi-spaces down to single spaces\n",
    "        clean = clean.strip()\n",
    "        clean_tweets.append(clean)\n",
    "    return(clean_tweets)\n",
    "\n",
    "def find_duplicates(text):\n",
    "    cnt = Counter()\n",
    "    duplicates = [False] * len(text)\n",
    "    for i,tweet in enumerate(text):\n",
    "        cnt[tweet] += 1\n",
    "        if cnt[tweet] > 1:\n",
    "            duplicates[i] = True\n",
    "    return(duplicates)\n",
    "\n",
    "def find_short(text, min_len):\n",
    "    length = [len(tweet.split(' ')) for tweet in text]\n",
    "    too_short = [ln < min_len for ln in length]\n",
    "    return(too_short)\n",
    "\n",
    "def word_counter(df, text_col, terms):\n",
    "    df['length'] = [len(tweet.split(' ')) for tweet in df[text_col]]\n",
    "    for term in terms:\n",
    "        df[term] = [len(re.findall('\\\\b'+term+'\\\\b', tweet)) for tweet in df[text_col]]\n",
    "    return(df)\n",
    "\n",
    "def levenshtein(seq1, seq2):\n",
    "    # from: http://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "    #print (matrix)\n",
    "    return (matrix[size_x - 1, size_y - 1])\n",
    "\n",
    "def count_tweets(df, user_col):\n",
    "    ## count number of tweets by each user\n",
    "    cnt = Counter()\n",
    "    for tweet in df[user_col]: ## change df_1 to count_df\n",
    "        cnt[tweet] += 1\n",
    "\n",
    "    keys = []\n",
    "    values = []\n",
    "    for key,value in cnt.items():\n",
    "        keys.append(key)\n",
    "        values.append(value)\n",
    "    \n",
    "    user_df = pd.DataFrame({'user':keys,'count':values})\n",
    "    user_df = user_df.sort_values(by = 'count', ascending = False).reset_index(drop = True)\n",
    "    return(user_df)\n",
    "\n",
    "def avg_levens(user, df, user_col, text_col):\n",
    "    levens = []\n",
    "    user_tweets = list(df[df[user_col] == user][text_col])\n",
    "    for j in range(2 * len(user_tweets)):\n",
    "        samp = random.sample(user_tweets,2)\n",
    "        levens.append(levenshtein(samp[0],samp[1]) / np.mean([len(samp[0]),len(samp[1])]))\n",
    "    return(np.mean(levens))\n",
    "\n",
    "def cutoff_levs(user_df, clean_df, user_col, text_col, count_cutoff, lev_cutoff):\n",
    "    \n",
    "    all_levs = [inf] * user_df.shape[0]\n",
    "    for i in range(np.sum(user_df['count'] >= count_cutoff)):\n",
    "        lev = avg_levens(user_df['user'][i],clean_df,user_col,text_col)\n",
    "        all_levs[i] = lev\n",
    "            \n",
    "    user_df['lev'] = all_levs\n",
    "    remove_users = list(user_df[user_df['lev'] < lev_cutoff]['user'])\n",
    "    clean_df = clean_df[-clean_df.user_id.isin(remove_users)]\n",
    "    return(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_cleanser(db_name, cycle):\n",
    "    \n",
    "    # connect to the database\n",
    "    sqlite_file = db_name + '.sqlite'\n",
    "    conn = sqlite3.connect(sqlite_file)\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # get list of hashtags\n",
    "    tables = []\n",
    "    res = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    for name in res:\n",
    "        tables.append(name[0])\n",
    "    \n",
    "    if cycle == 0:\n",
    "        new_tables = tables\n",
    "    \n",
    "    if cycle > 0:\n",
    "        with open(\"tables.txt\", \"rb\") as fp:   # Unpickling\n",
    "            old_tables = pickle.load(fp)\n",
    "        new_tables = list(compress(tables,[tab not in old_tables for tab in tables]))\n",
    "            \n",
    "    with open(\"tables.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(tables, fp)\n",
    "    \n",
    "    all_counts = []\n",
    "    for table in new_tables:\n",
    "        # query data\n",
    "        df = query_table(c,table)\n",
    "\n",
    "        # add cleaner columns\n",
    "        df = trim_data(df,['tweet_id', 'user_id','user_followers','text'])\n",
    "        df['clean_text'] = clean_text(df['text'])\n",
    "        df['duplicate'] = find_duplicates(df['clean_text'])\n",
    "        df['short'] = find_short(df['clean_text'], min_words)\n",
    "\n",
    "        # trim data\n",
    "        clean_df = df[~df['duplicate'] & ~df['short']].drop(['duplicate','short'], axis = 1)\n",
    "        \n",
    "        user_df = count_tweets(clean_df, 'user_id')\n",
    "        clean_df = cutoff_levs(user_df, clean_df, 'user_id', 'clean_text', lev_if, lev_cutoff)\n",
    "\n",
    "        all_counts.append(clean_df)\n",
    "        \n",
    "    return(all_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for PDF Creation for Coders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(text,i,stopwords):\n",
    "    wordcloud = WordCloud(background_color='white',\n",
    "                      stopwords=stopwords,\n",
    "                      max_words=200,\n",
    "                      max_font_size=40,\n",
    "                      random_state=42\n",
    "                     ).generate(str(text))\n",
    "    fig = plt.figure(figsize=(0.5,0.25))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    plt.axis('off')\n",
    "    fig.savefig('saved_cloud'+str(i)+'.png', dpi=900)\n",
    "    plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_line(text, tweet_font, tweet_size, max_width):\n",
    "    new_text = str()\n",
    "    new_possibles = []\n",
    "    widths = []\n",
    "    for word in text.split(' '):\n",
    "        new_text += ' ' + word\n",
    "        new_text = new_text.strip()\n",
    "        new_possibles.append(new_text)\n",
    "        width = stringWidth(new_text, tweet_font, tweet_size)\n",
    "        widths.append(width)\n",
    "    checks = [wid < max_width for wid in widths]\n",
    "    if False in checks:\n",
    "        ind = checks.index(False)-1\n",
    "    else:\n",
    "        ind = len(widths)-1\n",
    "    line = new_possibles[ind]\n",
    "    rest = ' '.join(text.split(' ')[(ind+1):])\n",
    "    return(line,rest)\n",
    "\n",
    "def line_splitter(text, max_width, tweet_font, tweet_size):\n",
    "    num_lines = 1\n",
    "    line1 = text\n",
    "    lines = [line1]\n",
    "    tw = stringWidth(line1, tweet_font, tweet_size)\n",
    "    if tw > max_width:\n",
    "        num_lines += 1\n",
    "        line1, rest = split_line(line1, tweet_font, tweet_size, max_width)\n",
    "        lines = [line1,rest]\n",
    "        tw = stringWidth(rest, tweet_font, tweet_size)\n",
    "        if tw > max_width:\n",
    "            num_lines += 1\n",
    "            line2, line3 = split_line(rest, tweet_font, tweet_size, max_width)\n",
    "            lines = [line1,line2,line3]\n",
    "            tw = stringWidth(line3, tweet_font, tweet_size)\n",
    "            if tw > max_width:\n",
    "                num_lines = 0\n",
    "    if num_lines > 0:\n",
    "        return(lines)\n",
    "    else:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pdf(tweet_data, pdf_name, minimum_tweets, queries, cycle):\n",
    "    cnvs = canvas.Canvas(pdf_name + str(cycle) + \".pdf\", pagesize=letter)\n",
    "\n",
    "    stopwords = set(list(STOPWORDS) + ownership_dictionary)\n",
    "    re_owner = '(\\\\b'+'\\\\b)|(\\\\b'.join(ownership_dictionary)+'\\\\b)'\n",
    "\n",
    "    # only look at hashtags that have enough tweets\n",
    "    counts = []\n",
    "    for hashtag in tweet_data:\n",
    "        counts.append(hashtag.shape[0])\n",
    "\n",
    "    enough = [count > minimum_tweets for count in counts]\n",
    "    good_hashtags = list(compress(queries, enough))\n",
    "    good_data = list(compress(tweet_data, enough))\n",
    "\n",
    "    ## settings for pdf writing\n",
    "    #from reportlab.lib.units import inch, cm\n",
    "    w, h = letter\n",
    "    tweet_font = 'Helvetica'\n",
    "    tweet_size = 16\n",
    "\n",
    "    for i in range(len(good_hashtags)):\n",
    "   \n",
    "        ## create word cloud\n",
    "        ## based on https://www.kaggle.com/adiljadoon/word-cloud-with-python\n",
    "        create_wordcloud(good_data[i]['clean_text'],i,stopwords)\n",
    "\n",
    "        tweet_samples = random.sample(list(good_data[i]['text']),minimum_tweets)\n",
    "        clean_samples = []\n",
    "        for sample in tweet_samples:\n",
    "            sample = re.sub('\\n','',sample)\n",
    "            sample = re.sub('http[^ ]*','',sample)\n",
    "            sample = re.sub('\"',\"'\",sample)\n",
    "            sample = re.sub('&amp;','&',sample)\n",
    "            sample = re.sub(\"[^0-9a-zA-Z \\.#,';:<>/\\?~`!@$%^&\\*()-+=]\",'',sample)\n",
    "            sample = sample.strip()\n",
    "            clean_samples.append(sample)\n",
    "        \n",
    "        clean_samples = [sample for sample in clean_samples if not re.search(re_owner,sample.lower())]\n",
    "\n",
    "        ## create the pdf page\n",
    "        hashtag = re.sub('%23','#',good_hashtags[i])\n",
    "    \n",
    "        cnvs.setLineWidth(.3)\n",
    "        cnvs.setFont(tweet_font, 24)\n",
    " \n",
    "        ## write out the hashtag\n",
    "        ht_w = stringWidth(hashtag, tweet_font, 24)\n",
    "        cnvs.line(inch,h-inch,w-inch,h-inch)\n",
    "        cnvs.drawString(w/2-ht_w/2, h-inch+10, hashtag)\n",
    "\n",
    "        cnvs.setFont(tweet_font, tweet_size)\n",
    "        ## write out a tweet\n",
    "        max_width = w - inch/2\n",
    "        pos = 20\n",
    "        for sample in clean_samples:\n",
    "            if pos > 390:\n",
    "                break\n",
    "            lines = line_splitter(sample, max_width, tweet_font, tweet_size)\n",
    "            if lines != None:\n",
    "                for line in lines:\n",
    "                    t_w = stringWidth(line, tweet_font, tweet_size)\n",
    "                    cnvs.drawString((w-t_w)/2, h-inch-pos, line)\n",
    "                    pos += 16\n",
    "                pos += 20\n",
    "    \n",
    "        cnvs.drawImage(x=0.05*w,y=(h - pos - 0.35*h)/4,image='saved_cloud'+str(i)+'.png',\n",
    "                         width = 0.9*w, height = 0.35*h)\n",
    "        cnvs.showPage()\n",
    "    cnvs.save()\n",
    "\n",
    "    for i in range(len(good_hashtags)):\n",
    "        os.remove('saved_cloud'+str(i)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Run Everything for One Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(arg, cycle):\n",
    "    repeated_finder(places, woeids, ht_filename, reps, interval, cycle)\n",
    "    queries = score_hashtags(reps, ht_filename, hash_samples, cycle)\n",
    "    search(queries,db_name,tweet_samples)\n",
    "    search(queries[:large_hashtags],db_name+'_large',large_tweets)\n",
    "    all_counts = tweet_cleanser(db_name, cycle)\n",
    "    create_pdf(all_counts, pdf_name, minimum_tweets, queries, cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = '20180625_db'\n",
    "pdf_name = '20180625_pdf'\n",
    "ht_filename = '20180625_ht'\n",
    "reps = 12 \n",
    "interval = 60 * 60 \n",
    "hash_samples = 150\n",
    "tweet_samples = 10000\n",
    "minimum_tweets = 1000 ## fewest tweets allowed per hashtag\n",
    "min_words = 5 ## shortest allowed tweet\n",
    "ownership_dictionary = ['mine','my','your','yours','our','ours','their','theirs']\n",
    "lev_if = 5 ## number of tweets per user that will trigger a levenshtein check\n",
    "lev_cutoff = 0.7 ## minimum allowable levenshtein distance\n",
    "overall_reps = 7\n",
    "overall_interval = 60*60*24\n",
    "large_hashtags = 3\n",
    "large_tweets = 100000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually Run The Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(5 * 60 + 11 * 60 * 60)\n",
    "\n",
    "places, woeids = get_woeids()\n",
    "\n",
    "for cycle in range(overall_reps):\n",
    "    thr = threading.Thread(target=run_all, args = (\"arg\", cycle)).start()\n",
    "    if cycle < (overall_reps - 1):\n",
    "        sleep(overall_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
